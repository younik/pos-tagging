{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ys_LD3OQc7Si"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 23/12/2021 (dd/mm/yyyy)\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this assignment  we will ask you to perform POS tagging using neural architectures\n",
        "\n",
        "You are asked to follow these steps:\n",
        "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "*   Embed the words using GloVe embeddings\n",
        "*   Create a baseline model, using a simple neural architecture\n",
        "*   Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
        "*   Evaluate your two best model\n",
        "*   Analyze the errors of your model\n",
        "\n",
        "\n",
        "**Task**: given a corpus of documents, predict the POS tag for each word\n",
        "\n",
        "**Corpus**:\n",
        "Ignore the numeric value in the third column, use only the words/symbols and its label. \n",
        "The corpus is available at:\n",
        "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "\n",
        "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
        "\n",
        "\n",
        "**Features**: you MUST use GloVe embeddings as the only input features to the model.\n",
        "\n",
        "**Splitting**: you can decide to split documents into sentences or not, the choice is yours.\n",
        "\n",
        "**I/O structure**: The input data will have three dimensions: 1-documents/sentences, 2-token, 3-features; for the output there are 2 possibilities: if you use one-hot encoding it will be 1-documents/sentences, 2-token labels, 3-classes, if you use a single integer that indicates the number of the class it will be 1-documents/sentences, 2-token labels.\n",
        "\n",
        "**Baseline**: two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top; the choice of hyper-parameters is yours.\n",
        "\n",
        "**Architectures**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and adding an additional dense layer; do not mix these variantions.\n",
        "\n",
        "\n",
        "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
        "\n",
        "**Evaluation**: in the end, only the two best models of your choice (according to the validation set) must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech. DO NOT CONSIDER THE PUNCTUATION CLASSES.\n",
        "\n",
        "**Metrics**: the metric you must use to evaluate your final model is the F1-macro, WITHOUT considering punctuation/symbols classes; during the training process you can use accuracy because you can't use the F1 metric unless you use a single (gigantic) batch because there is no way to aggregate \"partial\" F1 scores computed on mini-batches.\n",
        "\n",
        "**Discussion and Error Analysis** : verify and discuss if the results on the test sets are coherent with those on the validation set; analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
        "\n",
        "**Report**: you are asked to deliver the code of your experiments and a small pdf report of about 2 pages; the pdf must begin with the names of the people of your team and a small abstract (4-5 lines) that sums up your findings.\n",
        "\n",
        "# Out Of Vocabulary (OOV) terms\n",
        "\n",
        "How to handle words that are not in GloVe vocabulary?\n",
        "You can handle them as you want (random embedding, placeholder, whatever!), but they must be STATIC embeddings (you cannot train them).\n",
        "\n",
        "But there is a very important caveat! As usual, the element of the test set must not influence the elements of the other splits!\n",
        "\n",
        "So, when you compute new embeddings for train+validation, you must forget about test documents.\n",
        "The motivation is to emulate a real-world scenario, where you select and train a model in the first stage, without knowing nothing about the testing environment.\n",
        "\n",
        "For implementation convenience, you CAN use a single vocabulary file/matrix/whatever. The principle of the previous point is that the embeddings inside that file/matrix must be generated independently for train and test splits.\n",
        "\n",
        "Basically in a real-world scenario, this is what would happen:\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Training of the model(s)\n",
        "5. Compute embeddings for terms OOV2 of the validation split \n",
        "6. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "7. Validation of the model(s)\n",
        "8. Compute embeddings for terms OOV3 of the test split \n",
        "9. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
        "10. Testing of the final model\n",
        "\n",
        "In this case, where we already have all the documents, we can simplify the process a bit, but the procedure must remain rigorous.\n",
        "\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Compute embeddings for terms OOV2 of the validation split \n",
        "5. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "6. Compute embeddings for terms OOV3 of the test split \n",
        "7. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2\n",
        "8. Training of the model(s)\n",
        "9. Validation of the model(s)\n",
        "10. Testing of the final model\n",
        "\n",
        "Step 2 and step 6 must be completely independent of each other, for what concerns the method and the documents. But they can rely on the previous vocabulary (V1 for step 2 and V3 for step 6)\n",
        "THEREFORE if a word is present both in the training set and the test split and not in the starting vocabulary, its embedding is computed in step 2) and it is not considered OOV anymore in step 6).\n",
        "\n",
        "# Report\n",
        "The report must not be just a copy and paste of graphs and tables!\n",
        "\n",
        "The report must not be longer than 2 pages and must contain:\n",
        "* The names of the member of your team\n",
        "* A short abstract (4-5 lines) that sum ups everything\n",
        "* A general description of the task you have addressed and how you have addressed it\n",
        "* A short description of the models you have used\n",
        "* Some tables that sum up your findings in validation and test and a discussion of those results\n",
        "* The most relevant findings of your error analysis\n",
        "\n",
        "# Evaluation Criterion\n",
        "\n",
        "The goal of this assignment is not to prove you can find best model ever, but to face a common task, structure it correctly, and follow a correct and rigorous experimental procedure.\n",
        "In other words, we don't care if you final models are awful as long as you have followed the correct procedure and wrote a decent report.\n",
        "\n",
        "The score of the assignment will be computed roughly as follows\n",
        "* 1 point for the general setting of the problem\n",
        "* 1 point for the handling of OOV terms\n",
        "* 1 point for the models\n",
        "* 1 point for train-validation-test procedure\n",
        "* 2 point for the discussion of the results, error analysis, and report\n",
        "\n",
        "This distribution of scores is tentative and we may decide to alter it at any moment.\n",
        "We also reserve the right to assign a small bonus (0.5 points) to any assignment that is particularly worthy. Similarly, in case of grave errors, we may decide to assign an equivalent malus (-0.5 points).\n",
        "\n",
        "# Contacts\n",
        "\n",
        "In case of any doubt, question, issue, or help we highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1036039/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information, and to use the Virtuale forums to discuss with other students.\n",
        "\n",
        "You can always contact us at the following email addresses. To increase the probability of a prompt response, we reccomend you to write to both the teaching assistants.\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "\n",
        "# FAQ\n",
        "* You can use a non-trainable Embedding layer to load the glove embeddings\n",
        "* You can use any library of your choice to implement the networks. Two options are tensorflow/keras or pythorch. Both these libraries have all the classes you need to implement these simple architectures and there are plenty of tutorials around, where you can learn how to use them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8PgAJyy40SY"
      },
      "source": [
        "### Import libraries & download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caNU8UeFxupB"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2j0JgX8wpjH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "233e40ac-35b0-4e39-c4b9-8b96ac100be0"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "!unzip -q dependency_treebank.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-02 22:23:39--  https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 457429 (447K) [application/zip]\n",
            "Saving to: ‘dependency_treebank.zip’\n",
            "\n",
            "\rdependency_treebank   0%[                    ]       0  --.-KB/s               \rdependency_treebank 100%[===================>] 446.71K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-12-02 22:23:39 (12.1 MB/s) - ‘dependency_treebank.zip’ saved [457429/457429]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onQNYcuwCTiW"
      },
      "source": [
        "### Dataframe creation and split into train, val and test set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EADZWC3txls8"
      },
      "source": [
        "corpus_path = 'dependency_treebank/'\n",
        "file_list = sorted(os.listdir(corpus_path))\n",
        "\n",
        "dfs = [pd.read_csv(corpus_path + f, sep='\\t', names=['word', 'label', 'foo']).drop('foo', 1)\n",
        "          for f in file_list]\n",
        "\n",
        "dataset = pd.concat(dfs, ignore_index=True)  \n",
        "train_set = pd.concat(dfs[:100], ignore_index=True)\n",
        "val_set = pd.concat(dfs[100:150], ignore_index=True)\n",
        "test_set = pd.concat(dfs[150:199], ignore_index=True)\n",
        "\n",
        "labels = dataset['label'].unique()\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNdKH1KQ7kKs",
        "outputId": "5ef4df35-f05f-4cd1-fe67-eb3be3a3d94a"
      },
      "source": [
        "n_labels = len(labels)\n",
        "print(f'There are {n_labels} labels')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 45 labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIntonFfnCa-"
      },
      "source": [
        "full_stops = train_set[train_set['label']=='.'].index\n",
        "max_length = max(full_stops[1:] - full_stops[:-1])\n",
        "max_length = max(max_length, full_stops[0]) #consider also the first sentence"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CX8b-FtBkG1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "15593cd6-7434-47ab-e65e-f6684f8936f6"
      },
      "source": [
        "#OneHotEncoding\n",
        "encoder = OneHotEncoder(handle_unknown='ignore',\n",
        "                        sparse = False,\n",
        "                        dtype = np.int32)\n",
        "\n",
        "y = np.array(train_set['label'], dtype = str).reshape(-1, 1)\n",
        "encoded_labels = encoder.fit_transform(y)\n",
        "\n",
        "encoded_labels = pd.DataFrame(encoded_labels)\n",
        "train_set = pd.concat([train_set, encoded_labels], axis=1)\n",
        "train_set.head()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>label</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>...</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pierre</td>\n",
              "      <td>NNP</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Vinken</td>\n",
              "      <td>NNP</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>61</td>\n",
              "      <td>CD</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>years</td>\n",
              "      <td>NNS</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 93 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     word label  0  1  2  3  4  5  6  7  ...  35  36  37  38  39  40  41  42  43  44\n",
              "0  Pierre   NNP  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0   0\n",
              "1  Vinken   NNP  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0   0\n",
              "2       ,     ,  0  0  0  1  0  0  0  0  ...   0   0   0   0   0   0   0   0   0   0\n",
              "3      61    CD  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0   0\n",
              "4   years   NNS  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0   0\n",
              "\n",
              "[5 rows x 93 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "R_9zxzgGSd6m",
        "outputId": "8f27eb10-61eb-496a-85ba-67fc90ee2ffe"
      },
      "source": [
        "#Ordinal Encoder\n",
        "encoder = OrdinalEncoder(dtype = np.int32)\n",
        "\n",
        "y = np.array(train_set['label'], dtype = str).reshape(-1, 1)\n",
        "encoded_labels = encoder.fit_transform(y)\n",
        "\n",
        "encoded_labels = pd.DataFrame(encoded_labels, columns=['Ord'])\n",
        "train_set = pd.concat([train_set, encoded_labels], axis=1)\n",
        "train_set.head()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>label</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>...</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>Ord</th>\n",
              "      <th>Ord</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pierre</td>\n",
              "      <td>NNP</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Vinken</td>\n",
              "      <td>NNP</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>61</td>\n",
              "      <td>CD</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>years</td>\n",
              "      <td>NNS</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 95 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     word label  0  1  2  3  4  5  6  ...  38  39  40  41  42  43  44  Ord  Ord\n",
              "0  Pierre   NNP  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   20   20\n",
              "1  Vinken   NNP  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   20   20\n",
              "2       ,     ,  0  0  0  1  0  0  0  ...   0   0   0   0   0   0   0    3    3\n",
              "3      61    CD  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0    9    9\n",
              "4   years   NNS  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   22   22\n",
              "\n",
              "[5 rows x 95 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5LMFMcs5ff8"
      },
      "source": [
        "There is a small problem with direct dialogue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DnWfZ4nvy6h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "c79c0ac4-4078-4770-fe08-ad59daa67614"
      },
      "source": [
        "first_sentence = [train_set['word'].iloc[i] for i in range(full_stops[0]+1)]\n",
        "sentences = [[train_set['word'].iloc[i] for i in range(full_stops[j]+1, full_stops[j+1]+1)] for j in range(len(full_stops)-1)]\n",
        "sentences.append(first_sentence)\n",
        "\n",
        "first_seq = [train_set['label'].iloc[i] for i in range(full_stops[0]+1)]\n",
        "seq_labels = [[train_set['label'].iloc[i] for i in range(full_stops[j]+1, full_stops[j+1]+1)] for j in range(len(full_stops)-1)]\n",
        "seq_labels.append(first_seq)\n",
        "\n",
        "#first_enc_seq = [train_set[train_set.columns[2:-1]].iloc[i] for i in range(full_stops.index[0]+1)]\n",
        "#enc_seq_labels = [[train_set[train_set.columns[2:-1]].iloc[i] for i in range(full_stops.index[j]+1, full_stops.index[j+1]+1)] for j in range(len(full_stops)-1)]\n",
        "#enc_seq_labels.append(first_seq)\n",
        "\n",
        "first_enc_seq = [train_set['Ord'].iloc[i] for i in range(full_stops[0]+1)]\n",
        "enc_seq_labels = [[train_set['Ord'].iloc[i] for i in range(full_stops[j]+1, full_stops[j+1]+1)] for j in range(len(full_stops)-1)]\n",
        "enc_seq_labels.append(first_seq)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-9f96abc2d2cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfirst_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_stops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_stops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_stops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_stops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfirst_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_stops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-9f96abc2d2cb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfirst_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_stops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_stops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_stops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_stops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfirst_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_stops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-9f96abc2d2cb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfirst_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_stops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_stops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_stops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_stops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfirst_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_stops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1433\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0;34m'key'\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ma\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m'axis'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m         \"\"\"\n\u001b[0;32m-> 1435\u001b[0;31m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1436\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlength\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mRangeIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \"\"\"\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsAgPfRqStWK"
      },
      "source": [
        "### Dataset analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT0pl2UG0H0O",
        "outputId": "bd394afc-0c7a-4cd6-826e-308de2b14974"
      },
      "source": [
        "max_length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys_LD3OQc7Si"
      },
      "source": [
        "#### Plot dataset distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsUG2Cee8vMU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "5124b80a-ab56-4b9d-c496-0eca96f0c68d"
      },
      "source": [
        "train_counts = train_set.groupby(by='label').count()\n",
        "train_counts = train_counts / len(train_set) * 100\n",
        "val_counts = val_set.groupby(by='label').count()\n",
        "val_counts = val_counts / len(val_set) * 100\n",
        "val_counts = val_counts.reindex(labels, fill_value=0)\n",
        "test_counts = test_set.groupby(by='label').count()\n",
        "test_counts = test_counts / len(test_set) * 100\n",
        "test_counts = test_counts.reindex(labels, fill_value=0)\n",
        "ind = np.arange(len(labels))\n",
        "\n",
        "plt.figure(figsize=(18,6))\n",
        "width = 0.2       \n",
        "plt.bar(ind, train_counts.loc[labels, 'word'] , width, label='Train set');\n",
        "plt.bar(ind + width, val_counts.loc[labels, 'word'], width, label='Validation set');\n",
        "plt.bar(ind + 2*width, test_counts.loc[labels, 'word'], width, label='Test set');\n",
        "\n",
        "plt.xlabel('Labels');\n",
        "plt.ylabel('Count %');\n",
        "plt.title('Dataset distribution');\n",
        "\n",
        "plt.xticks(ind + width / 3, labels, rotation=45);\n",
        "plt.legend();"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpuY1KPGZzKn"
      },
      "source": [
        "counts_word = train_set.groupby('word').count().to_numpy()[:, 0]\n",
        "\n",
        "plt.plot(np.sort(counts_word)[::-1]);\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.xlabel('Words');\n",
        "plt.ylabel('Count');\n",
        "plt.title('Word distribution (log scale)');"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpBgAWGoje7b"
      },
      "source": [
        "### Downloading GloVe and creating the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7YehS9T9aWO"
      },
      "source": [
        "# Funzioni copiate dai tutorial, possono essere utili per scaricare al volo glove, e per la gestione dei vocabulary\n",
        "\n",
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "\n",
        "from collections import OrderedDict\n",
        "from tqdm import tqdm\n",
        "\n",
        "from typing import  Dict, List\n",
        "\n",
        "# Function definition\n",
        "def build_vocabulary(df: pd.DataFrame) -> (Dict[int, str],\n",
        "                                           Dict[str, int],\n",
        "                                           List[str]):\n",
        "    \"\"\"\n",
        "    Given a dataset, builds the corresponding word vocabulary.\n",
        "\n",
        "    :param df: dataset from which we want to build the word vocabulary (pandas.DataFrame)\n",
        "    :return:\n",
        "      - word vocabulary: vocabulary index to word\n",
        "      - inverse word vocabulary: word to vocabulary index\n",
        "      - word listing: set of unique terms that build up the vocabulary\n",
        "    \"\"\"\n",
        "\n",
        "    idx_to_word = OrderedDict()\n",
        "    word_to_idx = OrderedDict()\n",
        "    \n",
        "    curr_idx = 0\n",
        "    for token in tqdm(df.word.values):\n",
        "        if token not in word_to_idx:\n",
        "            word_to_idx[token] = curr_idx\n",
        "            idx_to_word[curr_idx] = token\n",
        "            curr_idx += 1\n",
        "\n",
        "    word_listing = list(idx_to_word.values())\n",
        "\n",
        "    return idx_to_word, word_to_idx, word_listing\n",
        "\n",
        "def load_glove_embedding(embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
        "    \"\"\"\n",
        "    Loads a pre-trained word embedding model via gensim library.\n",
        "\n",
        "    :param embedding_dimension: size of the embedding space to consider\n",
        "\n",
        "    :return\n",
        "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
        "    \"\"\"\n",
        "\n",
        "    download_path = f'glove-wiki-gigaword-{embedding_dimension}'\n",
        "\n",
        "    # Check download\n",
        "    try:\n",
        "        emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "        print(\"Glove: 50, 100, 200, 300\")\n",
        "        raise e\n",
        "\n",
        "    return emb_model\n",
        "\n",
        "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
        "                    word_listing: List[str]):\n",
        "    \"\"\"\n",
        "    Checks differences between pre-trained embedding model vocabulary\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_listing: dataset specific vocabulary (list)\n",
        "\n",
        "    :return\n",
        "        - list of OOV terms\n",
        "    \"\"\"\n",
        "\n",
        "    embedding_vocabulary = set(embedding_model.vocab.keys())\n",
        "    oov = set(word_listing).difference(embedding_vocabulary)\n",
        "    return list(oov)\n",
        "\n",
        "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
        "                           embedding_dimension: int,\n",
        "                           word_to_idx: Dict[str, int],\n",
        "                           oov_terms: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    vocab_size = len(word_to_idx)\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
        "\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "        try:\n",
        "            embedding_vector = embedding_model[word]\n",
        "        except (KeyError, TypeError):\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1HPEHFF9bOz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b333d2ed-95ae-4885-a1f7-91b1074a0b3c"
      },
      "source": [
        "# Esempi di utilizzo\n",
        "\n",
        "train_id_w, train_w_id, train_words = build_vocabulary(train_set)\n",
        "val_id_w, val_w_id, val_words = build_vocabulary(val_set)\n",
        "test_id_w, test_w_id, test_words = build_vocabulary(test_set)\n",
        "\n",
        "print()\n",
        "print(f'Train: {len(train_words)} words')\n",
        "print(f'Validation: {len(val_words)} words')\n",
        "print(f'Test: {len(test_words)} words')\n",
        "\n",
        "embedding_dimension = 50\n",
        "\n",
        "glove_50 = load_glove_embedding(embedding_dimension=50)\n",
        "\n",
        "oov_terms = check_OOV_terms(glove_50, test_words)\n",
        "\n",
        "ovperc = len(oov_terms)/len(train_words) * 100\n",
        "print(f'Total OOV terms: {len(oov_terms)} ({ovperc:.2f}%)')\n",
        "\n",
        "embedding_matrix = build_embedding_matrix(glove_50, embedding_dimension,\n",
        "                                          train_w_id,\n",
        "                                          check_OOV_terms(glove_50, train_words))\n",
        "\n",
        "print(f'Embedding matrix shape: {embedding_matrix.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47356/47356 [00:00<00:00, 2185579.45it/s]\n",
            "100%|██████████| 31183/31183 [00:00<00:00, 1921390.63it/s]\n",
            "100%|██████████| 15545/15545 [00:00<00:00, 1944250.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train: 8009 words\n",
            "Validation: 5892 words\n",
            "Test: 3623 words\n",
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "Total OOV terms: 957 (11.95%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8009/8009 [00:00<00:00, 208328.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding matrix shape: (8009, 50)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54Mc72CUofKk"
      },
      "source": [
        "Glove has 3 .txt file that uses embeddings of different dimensions (50/100/200/300)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP3Fv3ZHjeJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fb9f16d-a905-4c2e-e10f-7adb4f6788cb"
      },
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-01 12:49:28--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-12-01 12:49:28--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  3.55MB/s    in 4m 55s  \n",
            "\n",
            "2021-12-01 12:54:24 (2.79 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_rYO2w5Dl8d",
        "outputId": "01f95229-2ce4-4200-a255-29cfd813b2aa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4JLMxCGsMWd"
      },
      "source": [
        "Train set is made of 47356  \n",
        "OOV1 has 7689 words  \n",
        "GloVe author affirms '*The pre-trained vectors do not have an unknown token, and currently the code just ignores out-of-vocabulary words when producing the co-occurrence counts*', so he suggests that'*average of all or a subset of the word vectors produces a good unknown vector*'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzdml7SBIxUH"
      },
      "source": [
        "glove_path = '/content/glove.6B.50d.txt'\n",
        "words = pd.read_csv(glove_path, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
        "\n",
        "def vec(w):\n",
        "  return words.loc[w].to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "_UVotbF-HKzb",
        "outputId": "8c75f044-a63c-4ee0-a88c-6be25fc806d5"
      },
      "source": [
        "words.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>0.418000</td>\n",
              "      <td>0.249680</td>\n",
              "      <td>-0.41242</td>\n",
              "      <td>0.12170</td>\n",
              "      <td>0.34527</td>\n",
              "      <td>-0.044457</td>\n",
              "      <td>-0.49688</td>\n",
              "      <td>-0.17862</td>\n",
              "      <td>-0.00066</td>\n",
              "      <td>-0.656600</td>\n",
              "      <td>0.278430</td>\n",
              "      <td>-0.147670</td>\n",
              "      <td>-0.55677</td>\n",
              "      <td>0.14658</td>\n",
              "      <td>-0.00951</td>\n",
              "      <td>0.011658</td>\n",
              "      <td>0.102040</td>\n",
              "      <td>-0.127920</td>\n",
              "      <td>-0.84430</td>\n",
              "      <td>-0.121810</td>\n",
              "      <td>-0.016801</td>\n",
              "      <td>-0.332790</td>\n",
              "      <td>-0.155200</td>\n",
              "      <td>-0.231310</td>\n",
              "      <td>-0.191810</td>\n",
              "      <td>-1.8823</td>\n",
              "      <td>-0.76746</td>\n",
              "      <td>0.099051</td>\n",
              "      <td>-0.421250</td>\n",
              "      <td>-0.19526</td>\n",
              "      <td>4.0071</td>\n",
              "      <td>-0.185940</td>\n",
              "      <td>-0.522870</td>\n",
              "      <td>-0.31681</td>\n",
              "      <td>0.000592</td>\n",
              "      <td>0.007445</td>\n",
              "      <td>0.17778</td>\n",
              "      <td>-0.158970</td>\n",
              "      <td>0.012041</td>\n",
              "      <td>-0.054223</td>\n",
              "      <td>-0.298710</td>\n",
              "      <td>-0.157490</td>\n",
              "      <td>-0.347580</td>\n",
              "      <td>-0.045637</td>\n",
              "      <td>-0.44251</td>\n",
              "      <td>0.187850</td>\n",
              "      <td>0.002785</td>\n",
              "      <td>-0.184110</td>\n",
              "      <td>-0.115140</td>\n",
              "      <td>-0.78581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>,</th>\n",
              "      <td>0.013441</td>\n",
              "      <td>0.236820</td>\n",
              "      <td>-0.16899</td>\n",
              "      <td>0.40951</td>\n",
              "      <td>0.63812</td>\n",
              "      <td>0.477090</td>\n",
              "      <td>-0.42852</td>\n",
              "      <td>-0.55641</td>\n",
              "      <td>-0.36400</td>\n",
              "      <td>-0.239380</td>\n",
              "      <td>0.130010</td>\n",
              "      <td>-0.063734</td>\n",
              "      <td>-0.39575</td>\n",
              "      <td>-0.48162</td>\n",
              "      <td>0.23291</td>\n",
              "      <td>0.090201</td>\n",
              "      <td>-0.133240</td>\n",
              "      <td>0.078639</td>\n",
              "      <td>-0.41634</td>\n",
              "      <td>-0.154280</td>\n",
              "      <td>0.100680</td>\n",
              "      <td>0.488910</td>\n",
              "      <td>0.312260</td>\n",
              "      <td>-0.125200</td>\n",
              "      <td>-0.037512</td>\n",
              "      <td>-1.5179</td>\n",
              "      <td>0.12612</td>\n",
              "      <td>-0.024420</td>\n",
              "      <td>-0.042961</td>\n",
              "      <td>-0.28351</td>\n",
              "      <td>3.5416</td>\n",
              "      <td>-0.119560</td>\n",
              "      <td>-0.014533</td>\n",
              "      <td>-0.14990</td>\n",
              "      <td>0.218640</td>\n",
              "      <td>-0.334120</td>\n",
              "      <td>-0.13872</td>\n",
              "      <td>0.318060</td>\n",
              "      <td>0.703580</td>\n",
              "      <td>0.448580</td>\n",
              "      <td>-0.080262</td>\n",
              "      <td>0.630030</td>\n",
              "      <td>0.321110</td>\n",
              "      <td>-0.467650</td>\n",
              "      <td>0.22786</td>\n",
              "      <td>0.360340</td>\n",
              "      <td>-0.378180</td>\n",
              "      <td>-0.566570</td>\n",
              "      <td>0.044691</td>\n",
              "      <td>0.30392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>.</th>\n",
              "      <td>0.151640</td>\n",
              "      <td>0.301770</td>\n",
              "      <td>-0.16763</td>\n",
              "      <td>0.17684</td>\n",
              "      <td>0.31719</td>\n",
              "      <td>0.339730</td>\n",
              "      <td>-0.43478</td>\n",
              "      <td>-0.31086</td>\n",
              "      <td>-0.44999</td>\n",
              "      <td>-0.294860</td>\n",
              "      <td>0.166080</td>\n",
              "      <td>0.119630</td>\n",
              "      <td>-0.41328</td>\n",
              "      <td>-0.42353</td>\n",
              "      <td>0.59868</td>\n",
              "      <td>0.288250</td>\n",
              "      <td>-0.115470</td>\n",
              "      <td>-0.041848</td>\n",
              "      <td>-0.67989</td>\n",
              "      <td>-0.250630</td>\n",
              "      <td>0.184720</td>\n",
              "      <td>0.086876</td>\n",
              "      <td>0.465820</td>\n",
              "      <td>0.015035</td>\n",
              "      <td>0.043474</td>\n",
              "      <td>-1.4671</td>\n",
              "      <td>-0.30384</td>\n",
              "      <td>-0.023441</td>\n",
              "      <td>0.305890</td>\n",
              "      <td>-0.21785</td>\n",
              "      <td>3.7460</td>\n",
              "      <td>0.004228</td>\n",
              "      <td>-0.184360</td>\n",
              "      <td>-0.46209</td>\n",
              "      <td>0.098329</td>\n",
              "      <td>-0.119070</td>\n",
              "      <td>0.23919</td>\n",
              "      <td>0.116100</td>\n",
              "      <td>0.417050</td>\n",
              "      <td>0.056763</td>\n",
              "      <td>-0.000064</td>\n",
              "      <td>0.068987</td>\n",
              "      <td>0.087939</td>\n",
              "      <td>-0.102850</td>\n",
              "      <td>-0.13931</td>\n",
              "      <td>0.223140</td>\n",
              "      <td>-0.080803</td>\n",
              "      <td>-0.356520</td>\n",
              "      <td>0.016413</td>\n",
              "      <td>0.10216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>0.708530</td>\n",
              "      <td>0.570880</td>\n",
              "      <td>-0.47160</td>\n",
              "      <td>0.18048</td>\n",
              "      <td>0.54449</td>\n",
              "      <td>0.726030</td>\n",
              "      <td>0.18157</td>\n",
              "      <td>-0.52393</td>\n",
              "      <td>0.10381</td>\n",
              "      <td>-0.175660</td>\n",
              "      <td>0.078852</td>\n",
              "      <td>-0.362160</td>\n",
              "      <td>-0.11829</td>\n",
              "      <td>-0.83336</td>\n",
              "      <td>0.11917</td>\n",
              "      <td>-0.166050</td>\n",
              "      <td>0.061555</td>\n",
              "      <td>-0.012719</td>\n",
              "      <td>-0.56623</td>\n",
              "      <td>0.013616</td>\n",
              "      <td>0.228510</td>\n",
              "      <td>-0.143960</td>\n",
              "      <td>-0.067549</td>\n",
              "      <td>-0.381570</td>\n",
              "      <td>-0.236980</td>\n",
              "      <td>-1.7037</td>\n",
              "      <td>-0.86692</td>\n",
              "      <td>-0.267040</td>\n",
              "      <td>-0.258900</td>\n",
              "      <td>0.17670</td>\n",
              "      <td>3.8676</td>\n",
              "      <td>-0.161300</td>\n",
              "      <td>-0.132730</td>\n",
              "      <td>-0.68881</td>\n",
              "      <td>0.184440</td>\n",
              "      <td>0.005246</td>\n",
              "      <td>-0.33874</td>\n",
              "      <td>-0.078956</td>\n",
              "      <td>0.241850</td>\n",
              "      <td>0.365760</td>\n",
              "      <td>-0.347270</td>\n",
              "      <td>0.284830</td>\n",
              "      <td>0.075693</td>\n",
              "      <td>-0.062178</td>\n",
              "      <td>-0.38988</td>\n",
              "      <td>0.229020</td>\n",
              "      <td>-0.216170</td>\n",
              "      <td>-0.225620</td>\n",
              "      <td>-0.093918</td>\n",
              "      <td>-0.80375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>0.680470</td>\n",
              "      <td>-0.039263</td>\n",
              "      <td>0.30186</td>\n",
              "      <td>-0.17792</td>\n",
              "      <td>0.42962</td>\n",
              "      <td>0.032246</td>\n",
              "      <td>-0.41376</td>\n",
              "      <td>0.13228</td>\n",
              "      <td>-0.29847</td>\n",
              "      <td>-0.085253</td>\n",
              "      <td>0.171180</td>\n",
              "      <td>0.224190</td>\n",
              "      <td>-0.10046</td>\n",
              "      <td>-0.43653</td>\n",
              "      <td>0.33418</td>\n",
              "      <td>0.678460</td>\n",
              "      <td>0.057204</td>\n",
              "      <td>-0.344480</td>\n",
              "      <td>-0.42785</td>\n",
              "      <td>-0.432750</td>\n",
              "      <td>0.559630</td>\n",
              "      <td>0.100320</td>\n",
              "      <td>0.186770</td>\n",
              "      <td>-0.268540</td>\n",
              "      <td>0.037334</td>\n",
              "      <td>-2.0932</td>\n",
              "      <td>0.22171</td>\n",
              "      <td>-0.398680</td>\n",
              "      <td>0.209120</td>\n",
              "      <td>-0.55725</td>\n",
              "      <td>3.8826</td>\n",
              "      <td>0.474660</td>\n",
              "      <td>-0.956580</td>\n",
              "      <td>-0.37788</td>\n",
              "      <td>0.208690</td>\n",
              "      <td>-0.327520</td>\n",
              "      <td>0.12751</td>\n",
              "      <td>0.088359</td>\n",
              "      <td>0.163510</td>\n",
              "      <td>-0.216340</td>\n",
              "      <td>-0.094375</td>\n",
              "      <td>0.018324</td>\n",
              "      <td>0.210480</td>\n",
              "      <td>-0.030880</td>\n",
              "      <td>-0.19722</td>\n",
              "      <td>0.082279</td>\n",
              "      <td>-0.094340</td>\n",
              "      <td>-0.073297</td>\n",
              "      <td>-0.064699</td>\n",
              "      <td>-0.26044</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           1         2        3   ...        48        49       50\n",
              "0                                 ...                             \n",
              "the  0.418000  0.249680 -0.41242  ... -0.184110 -0.115140 -0.78581\n",
              ",    0.013441  0.236820 -0.16899  ... -0.566570  0.044691  0.30392\n",
              ".    0.151640  0.301770 -0.16763  ... -0.356520  0.016413  0.10216\n",
              "of   0.708530  0.570880 -0.47160  ... -0.225620 -0.093918 -0.80375\n",
              "to   0.680470 -0.039263  0.30186  ... -0.073297 -0.064699 -0.26044\n",
              "\n",
              "[5 rows x 50 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOYvhfJtGoXA"
      },
      "source": [
        "words.to_csv('/content/drive/MyDrive/NLP/Assignment1/GloVe/glove_50d.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExrJfgTms_my",
        "outputId": "8f164518-ea5b-4029-889c-08b7e2ec3763"
      },
      "source": [
        "vec('hello')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.38497 ,  0.80092 ,  0.064106, -0.28355 , -0.026759, -0.34532 ,\n",
              "       -0.64253 , -0.11729 , -0.33257 ,  0.55243 , -0.087813,  0.9035  ,\n",
              "        0.47102 ,  0.56657 ,  0.6985  , -0.35229 , -0.86542 ,  0.90573 ,\n",
              "        0.03576 , -0.071705, -0.12327 ,  0.54923 ,  0.47005 ,  0.35572 ,\n",
              "        1.2611  , -0.67581 , -0.94983 ,  0.68666 ,  0.3871  , -1.3492  ,\n",
              "        0.63512 ,  0.46416 , -0.48814 ,  0.83827 , -0.9246  , -0.33722 ,\n",
              "        0.53741 , -1.0616  , -0.081403, -0.67111 ,  0.30923 , -0.3923  ,\n",
              "       -0.55002 , -0.68827 ,  0.58049 , -0.11626 ,  0.013139, -0.57654 ,\n",
              "        0.048833,  0.67204 ])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0DHK_bbFSZd",
        "outputId": "984de238-c9cf-4428-84b5-03436229c1bb"
      },
      "source": [
        "glove_50_path = '/content/drive/MyDrive/NLP/Assignment1/GloVe/glove_50d.csv'\n",
        "glove_50 = pd.read_csv(glove_50_path)#, header=None, quoting=csv.QUOTE_NONE)\n",
        "\n",
        "print(f'Glove total length: {len(glove_50)}')\n",
        "print(f'Number of words in GloVe: {len(glove_50['0'].unique())}')\n",
        "\n",
        "glove_50 = glove_50.drop_duplicates(subset = '0', keep='first')\n",
        "print(f'Length after elimination: {}'.format(len(glove_50)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Glove total length: 400000\n",
            "Number of words in GloVe: 399998\n",
            "Length after elimination: 399998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmT4UdeDM_Gx"
      },
      "source": [
        "def OOV(vocabulary: pd.DataFrame, dataset: pd.DataFrame):\n",
        "  OOV = []\n",
        "  for word in dataset:\n",
        "    if word not in list(vocabulary):\n",
        "      OOV.append(word)\n",
        "\n",
        "  return OOV\n",
        "\n",
        "def oov_embeddings(vocabulary : pd.DataFrame):\n",
        "   return np.array([np.mean(vocabulary[str(i)]) for i in range(1, vocabulary.shape[1])])\n",
        "\n",
        "def new_vocabulary(old_vocabulary : pd.DataFrame, OOV: list, oov_embeddings: np.array):\n",
        "  \n",
        "  for word in OOV:\n",
        "    dict1 = {'0': word}\n",
        "    dict2 = {str(i+1) : oov_embeddings[i] for i in range(0, old_vocabulary.shape[1]-1)}\n",
        "    dict1.update(dict2)\n",
        "    old_vocabulary = old_vocabulary.append(dict1, ignore_index=True)\n",
        "\n",
        "  return old_vocabulary\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORuvPCXSAemT"
      },
      "source": [
        "print('Number of words in the train set : {}'.format(len(train_set['word'].unique())))\n",
        "\n",
        "OOV1 = OOV(glove_50['0'], train_set['word'].unique())\n",
        "\n",
        "print('Number of words OOV1 : {}'.format(len(OOV1)))\n",
        "\n",
        "oov1_embed = oov_embeddings(glove_50)\n",
        "V2 = new_vocabulary(glove_50, OOV1, oov1_embed)\n",
        "print('Number of words in V2 : {}'.format(len(V2)))\n",
        "\n",
        "OOV2 = OOV(V2['0'], val_set['word'].unique())\n",
        "print('Number of words OOV2 : {}'.format(len(OOV2)))\n",
        "\n",
        "oov2_embed = oov_embeddings(V2)\n",
        "V3 = new_vocabulary(V2, OOV2, oov2_embed)\n",
        "print('Number of words in V3 : {}'.format(len(V3)))\n",
        "\n",
        "OOV3 = OOV(V3['0'], test_set['word'].unique())\n",
        "print('Number of words OOV3 : {}'.format(len(OOV3)))\n",
        "\n",
        "oov3_embed = oov_embeddings(V3)\n",
        "V4 = new_vocabulary(V3, OOV3, oov3_embed)\n",
        "print('Number of words in V4 : {}'.format(len(V4)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa9eCkjMwHsO"
      },
      "source": [
        "### Create RNN and Optimizer classes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qKGsuBvwKk6"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_size, output_size, layers, dropout, device):\n",
        "    super(RNN, self).__init__()\n",
        "    self.n_layers = layers\n",
        "    self.output_size = output_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.device = device\n",
        "    self.rnn = nn.LSTM(input_dim, hidden_size, num_layers=self.n_layers, dropout=dropout, bidirectional=True)\n",
        "    self.fc = nn.Linear(self.n_layers*self.hidden_size, output_size)\n",
        "\n",
        "\n",
        "  def forward(self, input):\n",
        "    #initialize hidden state and cell state\n",
        "    hidden = (torch.randn(self.n_layers, input.shape[1],\n",
        "                          self.hidden_size).to(self.device),\n",
        "              torch.randn(self.n_layers, input.shape[1],\n",
        "                          self.hidden_size).to(self.device))\n",
        "\n",
        "\n",
        "    output, hidden = self.lstm(input, hidden)\n",
        "\n",
        "    h_n = hidden[0].permute(1, 0, 2)\n",
        "    h_n = h_n.contiguous().view(h_n.shape[0], -1)\n",
        "\n",
        "    logits = self.fc(h_n)\n",
        "\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MCssLkB8TUg"
      },
      "source": [
        "class Optimization:\n",
        "    def __init__(self, model, loss_fn, optimizer):\n",
        "        self.model = model\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "    \n",
        "    def train_step(self, x, y):\n",
        "        self.model.train()\n",
        "        \n",
        "        yhat = self.model(x)\n",
        "        loss = self.loss_fn(y, yhat)\n",
        "        loss.backward()\n",
        "\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Moz4o3S4_YyY"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqCmp15-_o99"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}