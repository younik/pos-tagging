{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ys_LD3OQc7Si"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 23/12/2021 (dd/mm/yyyy)\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this assignment  we will ask you to perform POS tagging using neural architectures\n",
        "\n",
        "You are asked to follow these steps:\n",
        "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "*   Embed the words using GloVe embeddings\n",
        "*   Create a baseline model, using a simple neural architecture\n",
        "*   Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
        "*   Evaluate your two best model\n",
        "*   Analyze the errors of your model\n",
        "\n",
        "\n",
        "**Task**: given a corpus of documents, predict the POS tag for each word\n",
        "\n",
        "**Corpus**:\n",
        "Ignore the numeric value in the third column, use only the words/symbols and its label. \n",
        "The corpus is available at:\n",
        "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "\n",
        "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
        "\n",
        "\n",
        "**Features**: you MUST use GloVe embeddings as the only input features to the model.\n",
        "\n",
        "**Splitting**: you can decide to split documents into sentences or not, the choice is yours.\n",
        "\n",
        "**I/O structure**: The input data will have three dimensions: 1-documents/sentences, 2-token, 3-features; for the output there are 2 possibilities: if you use one-hot encoding it will be 1-documents/sentences, 2-token labels, 3-classes, if you use a single integer that indicates the number of the class it will be 1-documents/sentences, 2-token labels.\n",
        "\n",
        "**Baseline**: two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top; the choice of hyper-parameters is yours.\n",
        "\n",
        "**Architectures**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and adding an additional dense layer; do not mix these variantions.\n",
        "\n",
        "\n",
        "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
        "\n",
        "**Evaluation**: in the end, only the two best models of your choice (according to the validation set) must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech. DO NOT CONSIDER THE PUNCTUATION CLASSES.\n",
        "\n",
        "**Metrics**: the metric you must use to evaluate your final model is the F1-macro, WITHOUT considering punctuation/symbols classes; during the training process you can use accuracy because you can't use the F1 metric unless you use a single (gigantic) batch because there is no way to aggregate \"partial\" F1 scores computed on mini-batches.\n",
        "\n",
        "**Discussion and Error Analysis** : verify and discuss if the results on the test sets are coherent with those on the validation set; analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
        "\n",
        "**Report**: you are asked to deliver the code of your experiments and a small pdf report of about 2 pages; the pdf must begin with the names of the people of your team and a small abstract (4-5 lines) that sums up your findings.\n",
        "\n",
        "# Out Of Vocabulary (OOV) terms\n",
        "\n",
        "How to handle words that are not in GloVe vocabulary?\n",
        "You can handle them as you want (random embedding, placeholder, whatever!), but they must be STATIC embeddings (you cannot train them).\n",
        "\n",
        "But there is a very important caveat! As usual, the element of the test set must not influence the elements of the other splits!\n",
        "\n",
        "So, when you compute new embeddings for train+validation, you must forget about test documents.\n",
        "The motivation is to emulate a real-world scenario, where you select and train a model in the first stage, without knowing nothing about the testing environment.\n",
        "\n",
        "For implementation convenience, you CAN use a single vocabulary file/matrix/whatever. The principle of the previous point is that the embeddings inside that file/matrix must be generated independently for train and test splits.\n",
        "\n",
        "Basically in a real-world scenario, this is what would happen:\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Training of the model(s)\n",
        "5. Compute embeddings for terms OOV2 of the validation split \n",
        "6. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "7. Validation of the model(s)\n",
        "8. Compute embeddings for terms OOV3 of the test split \n",
        "9. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
        "10. Testing of the final model\n",
        "\n",
        "In this case, where we already have all the documents, we can simplify the process a bit, but the procedure must remain rigorous.\n",
        "\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Compute embeddings for terms OOV2 of the validation split \n",
        "5. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "6. Compute embeddings for terms OOV3 of the test split \n",
        "7. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2\n",
        "8. Training of the model(s)\n",
        "9. Validation of the model(s)\n",
        "10. Testing of the final model\n",
        "\n",
        "Step 2 and step 6 must be completely independent of each other, for what concerns the method and the documents. But they can rely on the previous vocabulary (V1 for step 2 and V3 for step 6)\n",
        "THEREFORE if a word is present both in the training set and the test split and not in the starting vocabulary, its embedding is computed in step 2) and it is not considered OOV anymore in step 6).\n",
        "\n",
        "# Report\n",
        "The report must not be just a copy and paste of graphs and tables!\n",
        "\n",
        "The report must not be longer than 2 pages and must contain:\n",
        "* The names of the member of your team\n",
        "* A short abstract (4-5 lines) that sum ups everything\n",
        "* A general description of the task you have addressed and how you have addressed it\n",
        "* A short description of the models you have used\n",
        "* Some tables that sum up your findings in validation and test and a discussion of those results\n",
        "* The most relevant findings of your error analysis\n",
        "\n",
        "# Evaluation Criterion\n",
        "\n",
        "The goal of this assignment is not to prove you can find best model ever, but to face a common task, structure it correctly, and follow a correct and rigorous experimental procedure.\n",
        "In other words, we don't care if you final models are awful as long as you have followed the correct procedure and wrote a decent report.\n",
        "\n",
        "The score of the assignment will be computed roughly as follows\n",
        "* 1 point for the general setting of the problem\n",
        "* 1 point for the handling of OOV terms\n",
        "* 1 point for the models\n",
        "* 1 point for train-validation-test procedure\n",
        "* 2 point for the discussion of the results, error analysis, and report\n",
        "\n",
        "This distribution of scores is tentative and we may decide to alter it at any moment.\n",
        "We also reserve the right to assign a small bonus (0.5 points) to any assignment that is particularly worthy. Similarly, in case of grave errors, we may decide to assign an equivalent malus (-0.5 points).\n",
        "\n",
        "# Contacts\n",
        "\n",
        "In case of any doubt, question, issue, or help we highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1036039/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information, and to use the Virtuale forums to discuss with other students.\n",
        "\n",
        "You can always contact us at the following email addresses. To increase the probability of a prompt response, we reccomend you to write to both the teaching assistants.\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "\n",
        "# FAQ\n",
        "* You can use a non-trainable Embedding layer to load the glove embeddings\n",
        "* You can use any library of your choice to implement the networks. Two options are tensorflow/keras or pythorch. Both these libraries have all the classes you need to implement these simple architectures and there are plenty of tutorials around, where you can learn how to use them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8PgAJyy40SY"
      },
      "source": [
        "### Import libraries & download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caNU8UeFxupB"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2j0JgX8wpjH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0016068a-ff3f-4b84-e8a9-1482a39c21ea"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "!unzip -q dependency_treebank.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-01 12:48:36--  https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 457429 (447K) [application/zip]\n",
            "Saving to: ‘dependency_treebank.zip’\n",
            "\n",
            "\rdependency_treebank   0%[                    ]       0  --.-KB/s               \rdependency_treebank 100%[===================>] 446.71K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2021-12-01 12:48:36 (55.3 MB/s) - ‘dependency_treebank.zip’ saved [457429/457429]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onQNYcuwCTiW"
      },
      "source": [
        "### Dataframe creation and split into train, val and test set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EADZWC3txls8"
      },
      "source": [
        "corpus_path = 'dependency_treebank/'\n",
        "file_list = sorted(os.listdir(corpus_path))\n",
        "\n",
        "dfs = [pd.read_csv(corpus_path + f, sep='\\t', names=['word', 'label', 'foo']).drop('foo', 1)\n",
        "          for f in file_list]\n",
        "\n",
        "dataset = pd.concat(dfs, ignore_index=True)  \n",
        "train_set = pd.concat(dfs[:100], ignore_index=True)\n",
        "val_set = pd.concat(dfs[100:150], ignore_index=True)\n",
        "test_set = pd.concat(dfs[150:199], ignore_index=True)\n",
        "\n",
        "labels = dataset['label'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsAgPfRqStWK"
      },
      "source": [
        "### Dataset analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIntonFfnCa-"
      },
      "source": [
        "full_stops = train_set[train_set['label']=='.']\n",
        "max_length = max(full_stops.index[1:] - full_stops.index[:-1])\n",
        "max_length = max(max_length, full_stops.index[0]) #consider also the first sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT0pl2UG0H0O",
        "outputId": "3ff91d89-1f95-4658-ba45-1384b720d5aa"
      },
      "source": [
        "max_length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys_LD3OQc7Si"
      },
      "source": [
        "#### Plot dataset distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "kB2qkpAFZyAs",
        "outputId": "18515cb7-f2f5-4a54-976b-73e2b6e8104f"
      },
      "source": [
        "train_counts = train_set.groupby(by='label').count()\n",
        "val_counts = val_set.groupby(by='label').count()\n",
        "val_counts = val_counts.reindex(labels, fill_value=0)\n",
        "test_counts = test_set.groupby(by='label').count()\n",
        "test_counts = test_counts.reindex(labels, fill_value=0)\n",
        "ind = np.arange(len(labels))\n",
        "\n",
        "plt.figure(figsize=(18,6))\n",
        "width = 0.2       \n",
        "plt.bar(ind, train_counts.loc[labels, 'word'] , width, label='Train set');\n",
        "plt.bar(ind + width, val_counts.loc[labels, 'word'], width, label='Validation set');\n",
        "plt.bar(ind + 2*width, test_counts.loc[labels, 'word'], width, label='Test set');\n",
        "\n",
        "plt.xlabel('Labels');\n",
        "plt.ylabel('Count');\n",
        "plt.title('Dataset distribution');\n",
        "plt.xticks(ind + width / 3, labels, rotation=45);\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpBgAWGoje7b"
      },
      "source": [
        "### Downloading GloVe and creating the embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54Mc72CUofKk"
      },
      "source": [
        "Glove has 3 .txt file that uses embeddings of different dimensions (50/100/200/300)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP3Fv3ZHjeJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fb9f16d-a905-4c2e-e10f-7adb4f6788cb"
      },
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-01 12:49:28--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-12-01 12:49:28--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  3.55MB/s    in 4m 55s  \n",
            "\n",
            "2021-12-01 12:54:24 (2.79 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_rYO2w5Dl8d",
        "outputId": "01f95229-2ce4-4200-a255-29cfd813b2aa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4JLMxCGsMWd"
      },
      "source": [
        "Train set is made of 47356  \n",
        "OOV1 has 7689 words  \n",
        "GloVe author affirms '*The pre-trained vectors do not have an unknown token, and currently the code just ignores out-of-vocabulary words when producing the co-occurrence counts*', so he suggests that'*average of all or a subset of the word vectors produces a good unknown vector*'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzdml7SBIxUH"
      },
      "source": [
        "glove_path = '/content/glove.6B.50d.txt'\n",
        "words = pd.read_csv(glove_path, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
        "\n",
        "def vec(w):\n",
        "  return words.loc[w].to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "_UVotbF-HKzb",
        "outputId": "8c75f044-a63c-4ee0-a88c-6be25fc806d5"
      },
      "source": [
        "words.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>0.418000</td>\n",
              "      <td>0.249680</td>\n",
              "      <td>-0.41242</td>\n",
              "      <td>0.12170</td>\n",
              "      <td>0.34527</td>\n",
              "      <td>-0.044457</td>\n",
              "      <td>-0.49688</td>\n",
              "      <td>-0.17862</td>\n",
              "      <td>-0.00066</td>\n",
              "      <td>-0.656600</td>\n",
              "      <td>0.278430</td>\n",
              "      <td>-0.147670</td>\n",
              "      <td>-0.55677</td>\n",
              "      <td>0.14658</td>\n",
              "      <td>-0.00951</td>\n",
              "      <td>0.011658</td>\n",
              "      <td>0.102040</td>\n",
              "      <td>-0.127920</td>\n",
              "      <td>-0.84430</td>\n",
              "      <td>-0.121810</td>\n",
              "      <td>-0.016801</td>\n",
              "      <td>-0.332790</td>\n",
              "      <td>-0.155200</td>\n",
              "      <td>-0.231310</td>\n",
              "      <td>-0.191810</td>\n",
              "      <td>-1.8823</td>\n",
              "      <td>-0.76746</td>\n",
              "      <td>0.099051</td>\n",
              "      <td>-0.421250</td>\n",
              "      <td>-0.19526</td>\n",
              "      <td>4.0071</td>\n",
              "      <td>-0.185940</td>\n",
              "      <td>-0.522870</td>\n",
              "      <td>-0.31681</td>\n",
              "      <td>0.000592</td>\n",
              "      <td>0.007445</td>\n",
              "      <td>0.17778</td>\n",
              "      <td>-0.158970</td>\n",
              "      <td>0.012041</td>\n",
              "      <td>-0.054223</td>\n",
              "      <td>-0.298710</td>\n",
              "      <td>-0.157490</td>\n",
              "      <td>-0.347580</td>\n",
              "      <td>-0.045637</td>\n",
              "      <td>-0.44251</td>\n",
              "      <td>0.187850</td>\n",
              "      <td>0.002785</td>\n",
              "      <td>-0.184110</td>\n",
              "      <td>-0.115140</td>\n",
              "      <td>-0.78581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>,</th>\n",
              "      <td>0.013441</td>\n",
              "      <td>0.236820</td>\n",
              "      <td>-0.16899</td>\n",
              "      <td>0.40951</td>\n",
              "      <td>0.63812</td>\n",
              "      <td>0.477090</td>\n",
              "      <td>-0.42852</td>\n",
              "      <td>-0.55641</td>\n",
              "      <td>-0.36400</td>\n",
              "      <td>-0.239380</td>\n",
              "      <td>0.130010</td>\n",
              "      <td>-0.063734</td>\n",
              "      <td>-0.39575</td>\n",
              "      <td>-0.48162</td>\n",
              "      <td>0.23291</td>\n",
              "      <td>0.090201</td>\n",
              "      <td>-0.133240</td>\n",
              "      <td>0.078639</td>\n",
              "      <td>-0.41634</td>\n",
              "      <td>-0.154280</td>\n",
              "      <td>0.100680</td>\n",
              "      <td>0.488910</td>\n",
              "      <td>0.312260</td>\n",
              "      <td>-0.125200</td>\n",
              "      <td>-0.037512</td>\n",
              "      <td>-1.5179</td>\n",
              "      <td>0.12612</td>\n",
              "      <td>-0.024420</td>\n",
              "      <td>-0.042961</td>\n",
              "      <td>-0.28351</td>\n",
              "      <td>3.5416</td>\n",
              "      <td>-0.119560</td>\n",
              "      <td>-0.014533</td>\n",
              "      <td>-0.14990</td>\n",
              "      <td>0.218640</td>\n",
              "      <td>-0.334120</td>\n",
              "      <td>-0.13872</td>\n",
              "      <td>0.318060</td>\n",
              "      <td>0.703580</td>\n",
              "      <td>0.448580</td>\n",
              "      <td>-0.080262</td>\n",
              "      <td>0.630030</td>\n",
              "      <td>0.321110</td>\n",
              "      <td>-0.467650</td>\n",
              "      <td>0.22786</td>\n",
              "      <td>0.360340</td>\n",
              "      <td>-0.378180</td>\n",
              "      <td>-0.566570</td>\n",
              "      <td>0.044691</td>\n",
              "      <td>0.30392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>.</th>\n",
              "      <td>0.151640</td>\n",
              "      <td>0.301770</td>\n",
              "      <td>-0.16763</td>\n",
              "      <td>0.17684</td>\n",
              "      <td>0.31719</td>\n",
              "      <td>0.339730</td>\n",
              "      <td>-0.43478</td>\n",
              "      <td>-0.31086</td>\n",
              "      <td>-0.44999</td>\n",
              "      <td>-0.294860</td>\n",
              "      <td>0.166080</td>\n",
              "      <td>0.119630</td>\n",
              "      <td>-0.41328</td>\n",
              "      <td>-0.42353</td>\n",
              "      <td>0.59868</td>\n",
              "      <td>0.288250</td>\n",
              "      <td>-0.115470</td>\n",
              "      <td>-0.041848</td>\n",
              "      <td>-0.67989</td>\n",
              "      <td>-0.250630</td>\n",
              "      <td>0.184720</td>\n",
              "      <td>0.086876</td>\n",
              "      <td>0.465820</td>\n",
              "      <td>0.015035</td>\n",
              "      <td>0.043474</td>\n",
              "      <td>-1.4671</td>\n",
              "      <td>-0.30384</td>\n",
              "      <td>-0.023441</td>\n",
              "      <td>0.305890</td>\n",
              "      <td>-0.21785</td>\n",
              "      <td>3.7460</td>\n",
              "      <td>0.004228</td>\n",
              "      <td>-0.184360</td>\n",
              "      <td>-0.46209</td>\n",
              "      <td>0.098329</td>\n",
              "      <td>-0.119070</td>\n",
              "      <td>0.23919</td>\n",
              "      <td>0.116100</td>\n",
              "      <td>0.417050</td>\n",
              "      <td>0.056763</td>\n",
              "      <td>-0.000064</td>\n",
              "      <td>0.068987</td>\n",
              "      <td>0.087939</td>\n",
              "      <td>-0.102850</td>\n",
              "      <td>-0.13931</td>\n",
              "      <td>0.223140</td>\n",
              "      <td>-0.080803</td>\n",
              "      <td>-0.356520</td>\n",
              "      <td>0.016413</td>\n",
              "      <td>0.10216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>0.708530</td>\n",
              "      <td>0.570880</td>\n",
              "      <td>-0.47160</td>\n",
              "      <td>0.18048</td>\n",
              "      <td>0.54449</td>\n",
              "      <td>0.726030</td>\n",
              "      <td>0.18157</td>\n",
              "      <td>-0.52393</td>\n",
              "      <td>0.10381</td>\n",
              "      <td>-0.175660</td>\n",
              "      <td>0.078852</td>\n",
              "      <td>-0.362160</td>\n",
              "      <td>-0.11829</td>\n",
              "      <td>-0.83336</td>\n",
              "      <td>0.11917</td>\n",
              "      <td>-0.166050</td>\n",
              "      <td>0.061555</td>\n",
              "      <td>-0.012719</td>\n",
              "      <td>-0.56623</td>\n",
              "      <td>0.013616</td>\n",
              "      <td>0.228510</td>\n",
              "      <td>-0.143960</td>\n",
              "      <td>-0.067549</td>\n",
              "      <td>-0.381570</td>\n",
              "      <td>-0.236980</td>\n",
              "      <td>-1.7037</td>\n",
              "      <td>-0.86692</td>\n",
              "      <td>-0.267040</td>\n",
              "      <td>-0.258900</td>\n",
              "      <td>0.17670</td>\n",
              "      <td>3.8676</td>\n",
              "      <td>-0.161300</td>\n",
              "      <td>-0.132730</td>\n",
              "      <td>-0.68881</td>\n",
              "      <td>0.184440</td>\n",
              "      <td>0.005246</td>\n",
              "      <td>-0.33874</td>\n",
              "      <td>-0.078956</td>\n",
              "      <td>0.241850</td>\n",
              "      <td>0.365760</td>\n",
              "      <td>-0.347270</td>\n",
              "      <td>0.284830</td>\n",
              "      <td>0.075693</td>\n",
              "      <td>-0.062178</td>\n",
              "      <td>-0.38988</td>\n",
              "      <td>0.229020</td>\n",
              "      <td>-0.216170</td>\n",
              "      <td>-0.225620</td>\n",
              "      <td>-0.093918</td>\n",
              "      <td>-0.80375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>0.680470</td>\n",
              "      <td>-0.039263</td>\n",
              "      <td>0.30186</td>\n",
              "      <td>-0.17792</td>\n",
              "      <td>0.42962</td>\n",
              "      <td>0.032246</td>\n",
              "      <td>-0.41376</td>\n",
              "      <td>0.13228</td>\n",
              "      <td>-0.29847</td>\n",
              "      <td>-0.085253</td>\n",
              "      <td>0.171180</td>\n",
              "      <td>0.224190</td>\n",
              "      <td>-0.10046</td>\n",
              "      <td>-0.43653</td>\n",
              "      <td>0.33418</td>\n",
              "      <td>0.678460</td>\n",
              "      <td>0.057204</td>\n",
              "      <td>-0.344480</td>\n",
              "      <td>-0.42785</td>\n",
              "      <td>-0.432750</td>\n",
              "      <td>0.559630</td>\n",
              "      <td>0.100320</td>\n",
              "      <td>0.186770</td>\n",
              "      <td>-0.268540</td>\n",
              "      <td>0.037334</td>\n",
              "      <td>-2.0932</td>\n",
              "      <td>0.22171</td>\n",
              "      <td>-0.398680</td>\n",
              "      <td>0.209120</td>\n",
              "      <td>-0.55725</td>\n",
              "      <td>3.8826</td>\n",
              "      <td>0.474660</td>\n",
              "      <td>-0.956580</td>\n",
              "      <td>-0.37788</td>\n",
              "      <td>0.208690</td>\n",
              "      <td>-0.327520</td>\n",
              "      <td>0.12751</td>\n",
              "      <td>0.088359</td>\n",
              "      <td>0.163510</td>\n",
              "      <td>-0.216340</td>\n",
              "      <td>-0.094375</td>\n",
              "      <td>0.018324</td>\n",
              "      <td>0.210480</td>\n",
              "      <td>-0.030880</td>\n",
              "      <td>-0.19722</td>\n",
              "      <td>0.082279</td>\n",
              "      <td>-0.094340</td>\n",
              "      <td>-0.073297</td>\n",
              "      <td>-0.064699</td>\n",
              "      <td>-0.26044</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           1         2        3   ...        48        49       50\n",
              "0                                 ...                             \n",
              "the  0.418000  0.249680 -0.41242  ... -0.184110 -0.115140 -0.78581\n",
              ",    0.013441  0.236820 -0.16899  ... -0.566570  0.044691  0.30392\n",
              ".    0.151640  0.301770 -0.16763  ... -0.356520  0.016413  0.10216\n",
              "of   0.708530  0.570880 -0.47160  ... -0.225620 -0.093918 -0.80375\n",
              "to   0.680470 -0.039263  0.30186  ... -0.073297 -0.064699 -0.26044\n",
              "\n",
              "[5 rows x 50 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOYvhfJtGoXA"
      },
      "source": [
        "words.to_csv('/content/drive/MyDrive/NLP/Assignment1/GloVe/glove_50d.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExrJfgTms_my",
        "outputId": "8f164518-ea5b-4029-889c-08b7e2ec3763"
      },
      "source": [
        "vec('hello')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.38497 ,  0.80092 ,  0.064106, -0.28355 , -0.026759, -0.34532 ,\n",
              "       -0.64253 , -0.11729 , -0.33257 ,  0.55243 , -0.087813,  0.9035  ,\n",
              "        0.47102 ,  0.56657 ,  0.6985  , -0.35229 , -0.86542 ,  0.90573 ,\n",
              "        0.03576 , -0.071705, -0.12327 ,  0.54923 ,  0.47005 ,  0.35572 ,\n",
              "        1.2611  , -0.67581 , -0.94983 ,  0.68666 ,  0.3871  , -1.3492  ,\n",
              "        0.63512 ,  0.46416 , -0.48814 ,  0.83827 , -0.9246  , -0.33722 ,\n",
              "        0.53741 , -1.0616  , -0.081403, -0.67111 ,  0.30923 , -0.3923  ,\n",
              "       -0.55002 , -0.68827 ,  0.58049 , -0.11626 ,  0.013139, -0.57654 ,\n",
              "        0.048833,  0.67204 ])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0DHK_bbFSZd",
        "outputId": "984de238-c9cf-4428-84b5-03436229c1bb"
      },
      "source": [
        "glove_50_path = '/content/drive/MyDrive/NLP/Assignment1/GloVe/glove_50d.csv'\n",
        "glove_50 = pd.read_csv(glove_50_path)#, header=None, quoting=csv.QUOTE_NONE)\n",
        "\n",
        "print('Glove total length: {}'.format(len(glove_50)))\n",
        "print('Number of words in GloVe: {}'.format(len(glove_50['0'].unique())))\n",
        "\n",
        "glove_50 = glove_50.drop_duplicates(subset = '0', keep='first')\n",
        "print('Length after elimination: {}'.format(len(glove_50)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Glove total length: 400000\n",
            "Number of words in GloVe: 399998\n",
            "Length after elimination: 399998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "ajwhyGcbGIuh",
        "outputId": "18c3ae14-1f07-4d6a-e4e2-010fede5974a"
      },
      "source": [
        "glove_50.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the</td>\n",
              "      <td>0.418000</td>\n",
              "      <td>0.249680</td>\n",
              "      <td>-0.41242</td>\n",
              "      <td>0.12170</td>\n",
              "      <td>0.34527</td>\n",
              "      <td>-0.044457</td>\n",
              "      <td>-0.49688</td>\n",
              "      <td>-0.17862</td>\n",
              "      <td>-0.00066</td>\n",
              "      <td>-0.656600</td>\n",
              "      <td>0.278430</td>\n",
              "      <td>-0.147670</td>\n",
              "      <td>-0.55677</td>\n",
              "      <td>0.14658</td>\n",
              "      <td>-0.00951</td>\n",
              "      <td>0.011658</td>\n",
              "      <td>0.102040</td>\n",
              "      <td>-0.127920</td>\n",
              "      <td>-0.84430</td>\n",
              "      <td>-0.121810</td>\n",
              "      <td>-0.016801</td>\n",
              "      <td>-0.332790</td>\n",
              "      <td>-0.155200</td>\n",
              "      <td>-0.231310</td>\n",
              "      <td>-0.191810</td>\n",
              "      <td>-1.8823</td>\n",
              "      <td>-0.76746</td>\n",
              "      <td>0.099051</td>\n",
              "      <td>-0.421250</td>\n",
              "      <td>-0.19526</td>\n",
              "      <td>4.0071</td>\n",
              "      <td>-0.185940</td>\n",
              "      <td>-0.522870</td>\n",
              "      <td>-0.31681</td>\n",
              "      <td>0.000592</td>\n",
              "      <td>0.007445</td>\n",
              "      <td>0.17778</td>\n",
              "      <td>-0.158970</td>\n",
              "      <td>0.012041</td>\n",
              "      <td>-0.054223</td>\n",
              "      <td>-0.298710</td>\n",
              "      <td>-0.157490</td>\n",
              "      <td>-0.347580</td>\n",
              "      <td>-0.045637</td>\n",
              "      <td>-0.44251</td>\n",
              "      <td>0.187850</td>\n",
              "      <td>0.002785</td>\n",
              "      <td>-0.184110</td>\n",
              "      <td>-0.115140</td>\n",
              "      <td>-0.78581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>,</td>\n",
              "      <td>0.013441</td>\n",
              "      <td>0.236820</td>\n",
              "      <td>-0.16899</td>\n",
              "      <td>0.40951</td>\n",
              "      <td>0.63812</td>\n",
              "      <td>0.477090</td>\n",
              "      <td>-0.42852</td>\n",
              "      <td>-0.55641</td>\n",
              "      <td>-0.36400</td>\n",
              "      <td>-0.239380</td>\n",
              "      <td>0.130010</td>\n",
              "      <td>-0.063734</td>\n",
              "      <td>-0.39575</td>\n",
              "      <td>-0.48162</td>\n",
              "      <td>0.23291</td>\n",
              "      <td>0.090201</td>\n",
              "      <td>-0.133240</td>\n",
              "      <td>0.078639</td>\n",
              "      <td>-0.41634</td>\n",
              "      <td>-0.154280</td>\n",
              "      <td>0.100680</td>\n",
              "      <td>0.488910</td>\n",
              "      <td>0.312260</td>\n",
              "      <td>-0.125200</td>\n",
              "      <td>-0.037512</td>\n",
              "      <td>-1.5179</td>\n",
              "      <td>0.12612</td>\n",
              "      <td>-0.024420</td>\n",
              "      <td>-0.042961</td>\n",
              "      <td>-0.28351</td>\n",
              "      <td>3.5416</td>\n",
              "      <td>-0.119560</td>\n",
              "      <td>-0.014533</td>\n",
              "      <td>-0.14990</td>\n",
              "      <td>0.218640</td>\n",
              "      <td>-0.334120</td>\n",
              "      <td>-0.13872</td>\n",
              "      <td>0.318060</td>\n",
              "      <td>0.703580</td>\n",
              "      <td>0.448580</td>\n",
              "      <td>-0.080262</td>\n",
              "      <td>0.630030</td>\n",
              "      <td>0.321110</td>\n",
              "      <td>-0.467650</td>\n",
              "      <td>0.22786</td>\n",
              "      <td>0.360340</td>\n",
              "      <td>-0.378180</td>\n",
              "      <td>-0.566570</td>\n",
              "      <td>0.044691</td>\n",
              "      <td>0.30392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>.</td>\n",
              "      <td>0.151640</td>\n",
              "      <td>0.301770</td>\n",
              "      <td>-0.16763</td>\n",
              "      <td>0.17684</td>\n",
              "      <td>0.31719</td>\n",
              "      <td>0.339730</td>\n",
              "      <td>-0.43478</td>\n",
              "      <td>-0.31086</td>\n",
              "      <td>-0.44999</td>\n",
              "      <td>-0.294860</td>\n",
              "      <td>0.166080</td>\n",
              "      <td>0.119630</td>\n",
              "      <td>-0.41328</td>\n",
              "      <td>-0.42353</td>\n",
              "      <td>0.59868</td>\n",
              "      <td>0.288250</td>\n",
              "      <td>-0.115470</td>\n",
              "      <td>-0.041848</td>\n",
              "      <td>-0.67989</td>\n",
              "      <td>-0.250630</td>\n",
              "      <td>0.184720</td>\n",
              "      <td>0.086876</td>\n",
              "      <td>0.465820</td>\n",
              "      <td>0.015035</td>\n",
              "      <td>0.043474</td>\n",
              "      <td>-1.4671</td>\n",
              "      <td>-0.30384</td>\n",
              "      <td>-0.023441</td>\n",
              "      <td>0.305890</td>\n",
              "      <td>-0.21785</td>\n",
              "      <td>3.7460</td>\n",
              "      <td>0.004228</td>\n",
              "      <td>-0.184360</td>\n",
              "      <td>-0.46209</td>\n",
              "      <td>0.098329</td>\n",
              "      <td>-0.119070</td>\n",
              "      <td>0.23919</td>\n",
              "      <td>0.116100</td>\n",
              "      <td>0.417050</td>\n",
              "      <td>0.056763</td>\n",
              "      <td>-0.000064</td>\n",
              "      <td>0.068987</td>\n",
              "      <td>0.087939</td>\n",
              "      <td>-0.102850</td>\n",
              "      <td>-0.13931</td>\n",
              "      <td>0.223140</td>\n",
              "      <td>-0.080803</td>\n",
              "      <td>-0.356520</td>\n",
              "      <td>0.016413</td>\n",
              "      <td>0.10216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>of</td>\n",
              "      <td>0.708530</td>\n",
              "      <td>0.570880</td>\n",
              "      <td>-0.47160</td>\n",
              "      <td>0.18048</td>\n",
              "      <td>0.54449</td>\n",
              "      <td>0.726030</td>\n",
              "      <td>0.18157</td>\n",
              "      <td>-0.52393</td>\n",
              "      <td>0.10381</td>\n",
              "      <td>-0.175660</td>\n",
              "      <td>0.078852</td>\n",
              "      <td>-0.362160</td>\n",
              "      <td>-0.11829</td>\n",
              "      <td>-0.83336</td>\n",
              "      <td>0.11917</td>\n",
              "      <td>-0.166050</td>\n",
              "      <td>0.061555</td>\n",
              "      <td>-0.012719</td>\n",
              "      <td>-0.56623</td>\n",
              "      <td>0.013616</td>\n",
              "      <td>0.228510</td>\n",
              "      <td>-0.143960</td>\n",
              "      <td>-0.067549</td>\n",
              "      <td>-0.381570</td>\n",
              "      <td>-0.236980</td>\n",
              "      <td>-1.7037</td>\n",
              "      <td>-0.86692</td>\n",
              "      <td>-0.267040</td>\n",
              "      <td>-0.258900</td>\n",
              "      <td>0.17670</td>\n",
              "      <td>3.8676</td>\n",
              "      <td>-0.161300</td>\n",
              "      <td>-0.132730</td>\n",
              "      <td>-0.68881</td>\n",
              "      <td>0.184440</td>\n",
              "      <td>0.005246</td>\n",
              "      <td>-0.33874</td>\n",
              "      <td>-0.078956</td>\n",
              "      <td>0.241850</td>\n",
              "      <td>0.365760</td>\n",
              "      <td>-0.347270</td>\n",
              "      <td>0.284830</td>\n",
              "      <td>0.075693</td>\n",
              "      <td>-0.062178</td>\n",
              "      <td>-0.38988</td>\n",
              "      <td>0.229020</td>\n",
              "      <td>-0.216170</td>\n",
              "      <td>-0.225620</td>\n",
              "      <td>-0.093918</td>\n",
              "      <td>-0.80375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>to</td>\n",
              "      <td>0.680470</td>\n",
              "      <td>-0.039263</td>\n",
              "      <td>0.30186</td>\n",
              "      <td>-0.17792</td>\n",
              "      <td>0.42962</td>\n",
              "      <td>0.032246</td>\n",
              "      <td>-0.41376</td>\n",
              "      <td>0.13228</td>\n",
              "      <td>-0.29847</td>\n",
              "      <td>-0.085253</td>\n",
              "      <td>0.171180</td>\n",
              "      <td>0.224190</td>\n",
              "      <td>-0.10046</td>\n",
              "      <td>-0.43653</td>\n",
              "      <td>0.33418</td>\n",
              "      <td>0.678460</td>\n",
              "      <td>0.057204</td>\n",
              "      <td>-0.344480</td>\n",
              "      <td>-0.42785</td>\n",
              "      <td>-0.432750</td>\n",
              "      <td>0.559630</td>\n",
              "      <td>0.100320</td>\n",
              "      <td>0.186770</td>\n",
              "      <td>-0.268540</td>\n",
              "      <td>0.037334</td>\n",
              "      <td>-2.0932</td>\n",
              "      <td>0.22171</td>\n",
              "      <td>-0.398680</td>\n",
              "      <td>0.209120</td>\n",
              "      <td>-0.55725</td>\n",
              "      <td>3.8826</td>\n",
              "      <td>0.474660</td>\n",
              "      <td>-0.956580</td>\n",
              "      <td>-0.37788</td>\n",
              "      <td>0.208690</td>\n",
              "      <td>-0.327520</td>\n",
              "      <td>0.12751</td>\n",
              "      <td>0.088359</td>\n",
              "      <td>0.163510</td>\n",
              "      <td>-0.216340</td>\n",
              "      <td>-0.094375</td>\n",
              "      <td>0.018324</td>\n",
              "      <td>0.210480</td>\n",
              "      <td>-0.030880</td>\n",
              "      <td>-0.19722</td>\n",
              "      <td>0.082279</td>\n",
              "      <td>-0.094340</td>\n",
              "      <td>-0.073297</td>\n",
              "      <td>-0.064699</td>\n",
              "      <td>-0.26044</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0         1         2        3  ...        47        48        49       50\n",
              "0  the  0.418000  0.249680 -0.41242  ...  0.002785 -0.184110 -0.115140 -0.78581\n",
              "1    ,  0.013441  0.236820 -0.16899  ... -0.378180 -0.566570  0.044691  0.30392\n",
              "2    .  0.151640  0.301770 -0.16763  ... -0.080803 -0.356520  0.016413  0.10216\n",
              "3   of  0.708530  0.570880 -0.47160  ... -0.216170 -0.225620 -0.093918 -0.80375\n",
              "4   to  0.680470 -0.039263  0.30186  ... -0.094340 -0.073297 -0.064699 -0.26044\n",
              "\n",
              "[5 rows x 51 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmT4UdeDM_Gx"
      },
      "source": [
        "def oov_embeddings(vocabulary : pd.DataFrame):\n",
        "   return np.array([np.mean(vocabulary[str(i)]) for i in range(1, 51)])\n",
        "\n",
        "def new_vocabulary(old_vocabulary : pd.DataFrame, OOV: list, oov_embeddings: np.array):\n",
        "  for word in OOV:\n",
        "    dict1 = {'0': word}\n",
        "    dict2 = {dict2 = {str(i+1) : oov_embeddings[i] for i in range(0, vocabulary.shape[1])}}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAeCJHU0dFY2",
        "outputId": "ae7d345c-9b88-44f7-fd75-25e32ab9ecf8"
      },
      "source": [
        "print(glove_50.shape[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3JGjiRpPLAj",
        "outputId": "7049a7c2-6be6-46d3-b6f3-3f36b32f60a1"
      },
      "source": [
        "print('Number of words in the train set : {}'.format(len(train_set['word'].unique())))\n",
        "print('Number of words OOV : {}'.format(len(oov1)))\n",
        "\n",
        "OOV1 = []\n",
        "for word in train_set['word'].unique():\n",
        "  if word not in list(glove_50['0']):\n",
        "    OOV1.append(word)\n",
        "\n",
        "\n",
        "print(print('Number of words OOV : {}'.format(len(OOV1)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47356\n",
            "8009\n",
            "399998\n",
            "2346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZyj_VwLYWay"
      },
      "source": [
        "V2 = glove_50\n",
        "\n",
        "for word in OOV1:\n",
        "  dict1 = {'0': word}\n",
        "  dict2 = {str(i+1) : oov1_embeddings[i] for i in range(0, 50)}\n",
        "  dict1.update(dict2)\n",
        "  V2 = V2.append(dict1, ignore_index=True)\n",
        "\n",
        "print('Number of words in the new vocabulary V2 : {}'.format(len(V2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNdLnLvoYsCH",
        "outputId": "568fd271-8b77-4c01-b678-4febbdda175b"
      },
      "source": [
        "print(OOV1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                  Pierre\n",
            "1                  Vinken\n",
            "2                    Nov.\n",
            "3                     Mr.\n",
            "4                Elsevier\n",
            "              ...        \n",
            "2341    building-products\n",
            "2342             new-home\n",
            "2343      forest-products\n",
            "2344              Kathryn\n",
            "2345              McAuley\n",
            "Length: 2346, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHyLtWXRd29D",
        "outputId": "c0a09445-c287-473d-ed7c-53748ad781ed"
      },
      "source": [
        "print(train_set[train_set['word'] == 'new-home'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           word label\n",
            "47156  new-home    JJ\n",
            "47169  new-home    JJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4KgV5NXzg5O"
      },
      "source": [
        "def unk_embeddings(vocabulary):\n",
        "  #if os.path.exists(vocabulary): \n",
        "  # with open(vocabulary, 'r') as f:\n",
        "  #   for i, line in enumerate(f):\n",
        "  #        pass\n",
        "  #  n_words = i + 1\n",
        "  #  dimensions = len(line.split(' ')) - 1\n",
        "\n",
        "  #  vecs = np.zeros((n_vec, hidden_dim), dtype=np.float32)\n",
        "\n",
        "  #  with open(vocabulary, 'r') as f:\n",
        "  #    for i, line in enumerate(f):\n",
        "  #      vecs[i] = np.array([float(n) for n in line.split(' ')[1:]], dtype=np.float32)\n",
        "\n",
        "  #else:\n",
        "    n_words = len(vocabulary)\n",
        "    dimensions = vocabulary.values[0]\n",
        "\n",
        "    vecs = np.zeros((n_words, dimensions), dtype=np.float32)\n",
        "\n",
        "    for i, line in enumerate(f):\n",
        "        vecs[i] = np.array([float(n) for n in line.split(' ')[1:]], dtype=np.float32)\n",
        "\n",
        "    return np.mean(vecs, axis=0)\n",
        "\n",
        "\n",
        "def oov(vocabulary, dataset, average_vec):\n",
        "  for word in list(dataset):\n",
        "    if word not in vocabulary:\n",
        "      vocabulary[word] = average_vec\n",
        "\n",
        "  return vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa9eCkjMwHsO"
      },
      "source": [
        "### Create RNN and Optimizer classes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qKGsuBvwKk6"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_size, output_size, layers, dropout, device):\n",
        "    super(RNN, self).__init__()\n",
        "    self.n_layers = layers\n",
        "    self.output_size = output_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.device = device\n",
        "    self.rnn = nn.LSTM(input_dim, hidden_size, num_layers=self.n_layers, dropout=dropout)\n",
        "    self.fc = nn.Linear(self.n_layers*self.hidden_size, output_size)\n",
        "\n",
        "\n",
        "  def forward(self, input):\n",
        "    #initialize hidden state and cell state\n",
        "    hidden = (torch.randn(self.n_layers, input.shape[1],\n",
        "                          self.hidden_size).to(self.device),\n",
        "              torch.randn(self.n_layers, input.shape[1],\n",
        "                          self.hidden_size).to(self.device))\n",
        "\n",
        "\n",
        "    output, hidden = self.lstm(input, hidden)\n",
        "\n",
        "    h_n = hidden[0].permute(1, 0, 2)\n",
        "    h_n = h_n.contiguous().view(h_n.shape[0], -1)\n",
        "\n",
        "    logits = self.fc(h_n)\n",
        "\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MCssLkB8TUg"
      },
      "source": [
        "class Optimization:\n",
        "    def __init__(self, model, loss_fn, optimizer):\n",
        "        self.model = model\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "    \n",
        "    def train_step(self, x, y):\n",
        "        self.model.train()\n",
        "        \n",
        "        yhat = self.model(x)\n",
        "        loss = self.loss_fn(y, yhat)\n",
        "        loss.backward()\n",
        "\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Moz4o3S4_YyY"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqCmp15-_o99"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}